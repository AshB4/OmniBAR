# OmniBrew:Prompt Review Walkthrough Prep

This guide summarizes the points a reviewer is likely expecting in a five-minute walkthrough of OmniBrew:Prompt Review and offers phrasing you can adapt. Focus on three themes: *what the system does*, *how it works*, and *how it scales*. Use the talking points as a script but also as a checklist so the reviewer explicitly sees the latency, token, and payload transparency cues that differentiate OmniBrew from a bare-bones demo.

## 1. Frame the Review Objectives (30–45 seconds)
- **Set the context.** Remind the reviewer that OmniBrew:Prompt Review extends the OmniBAR Reliability Console so teams can measure latency, cost, accuracy, and completeness end-to-end. Mention that the demo covers the FastAPI backend, the React frontend, and the reliability tooling glueing them together.
- **Clarify the success criteria.** State that the reviewer is looking for: visible coverage of every core brief requirement, proof that the implementation is production-aware, and evidence that you have instrumentation for iteration (benchmarks, runs, document extraction lab).

## 2. Backend Deep Dive (90 seconds)
- **FastAPI surface area.** Walk through the autogenerated OpenAPI schema and highlight how the LatteRun and Benchmark models flow through each endpoint. Tie this to the live Swagger UI at `/docs`, noting that it stays in sync with the Pydantic schemas.
- **Operational extras.** Call out the mock scoring mode for offline testing and the Control Room endpoints that orchestrate benchmark snapshots, smoke tests, and reliability thresholds.
- **Reliability data.** Emphasize how benchmark suites, run history, and judge feedback expose the data a reviewer needs to trust the system.

Suggested phrasing:
> “FastAPI generates the full OpenAPI contract for us. I have it open in JSON Viewer Pro so you can see how LatteRun objects embed Benchmark summaries. That spec powers the interactive Swagger docs, which stay aligned with the Pydantic models we ship.”

## 3. Frontend and Reliability Console (90 seconds)
- **Control Room.** Demonstrate launching benchmark runs, adjusting threshold sliders, and accessing snapshot JSON for auditability.
- **Benchmarks view.** Show the summary cards, status badges, and JSON inspector that bridge both developer metrics (latency, cost, token counts) and scientist needs (raw outputs). Call out that every row links to a full JSON breakdown so reviewers can reproduce any score. Demonstrate the payload selector drop-down so the reviewer sees how multiple benchmark runs can be compared without leaving the view.
- **Runs log.** Explain how the run history provides regression tracking, smoke tests, and failure triage.
- **Error and loading UX.** Demonstrate the spinners, optimistic updates, and toast errors so reviewers see the guardrails when services degrade.
- **Document Extraction lab.** Point out the prompt strategy comparisons, radar chart, and judge feedback for structured extraction reliability.

Suggested phrasing:
> “Here’s the Control Room where operators kick off benchmark suites or trigger a smoke test. Notice the Suite Snapshot JSON panel—that’s how stakeholders can export the raw payload without digging into logs.”

## 4. Highlight Production Readiness (45 seconds)
- **Resilience.** Mention the connectivity checks, the ability to clear run history, and the knobs for thresholds.
- **Scalability.** Reference the mock mode and live Swagger UI as proof the system can run in different environments (offline QA versus integrated pipelines).
- **Observability.** Reinforce that every view exposes raw JSON plus judge feedback, enabling reproducible investigations. Note that OmniBrew:Prompt Review surfaces latency, token counts, and end-to-end payloads so nothing is hidden from reviewers. Highlight the Mock Mode badge in the shell header and explain how the same instrumentation works against staging or live backends.

Suggested phrasing:
> “Beyond the brief, I shipped Swagger for interactive exploration, mock scoring for offline runs, and the Control Room to centralize reliability testing. Those touches show how this scales from demo to production.”

## 5. Close with Next Steps (30 seconds)
- Summarize the core value: a reliability console that makes agent performance measurable and shareable.
- Offer the next iteration idea—e.g., wiring mock mode into CI, expanding benchmark libraries, or integrating alerting.
- Invite questions and point them to the Swagger docs or JSON snapshots if they want to inspect further.

## Quick Checklist Before the Walkthrough
- [ ] Swagger UI is running and populated with live schema.
- [ ] Mock mode flag demonstrated at least once (e.g., via environment toggle) and highlight the Mock Mode badge in the UI so the reviewer always knows the active environment.
- [ ] Control Room demo plan: launch a run, adjust threshold, export JSON.
- [ ] Benchmarks table data seeded so status badges show variety (pass/fail).
- [ ] Benchmarks payload selector pre-loaded with at least two runs so you can show the JSON inspector surfacing latency, token counts, and cost for each.
- [ ] Runs log includes at least one success and one failure to talk through, showing the loading states, error handling toasts, and the recovery flow.
- [ ] Document Extraction lab populated with contrasting strategies and judge notes.

Keep the narrative tight, but let the reviewer see and hear how each component contributes to reliable, scalable agent evaluation. End by reiterating that OmniBrew:Prompt Review is production-aware—mockable, observable, and auditable—so the reviewer can trust both the walkthrough and the underlying system.
